


Map (num_proc=8): 100%|███████████████████████████████████████████████| 500000/500000 [00:05<00:00, 88743.36 examples/s]
Casting the dataset: 100%|███████████████████████████████████████████| 500000/500000 [00:02<00:00, 205104.49 examples/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Snowflake/snowflake-arctic-embed-l and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.









































































Map:  28%|█████████████████▋                                            | 128000/450000 [02:28<06:14, 859.64 examples/s]
Traceback (most recent call last):
  File "/home/edward/Sila/train_edu_bert.py", line 192, in <module>
    main(args)
  File "/home/edward/Sila/train_edu_bert.py", line 105, in main
    dataset = dataset.map(preprocess, batched=True)
  File "/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py", line 866, in map
    {
  File "/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py", line 867, in <dictcomp>
    k: dataset.map(
  File "/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 560, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3035, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3438, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3300, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/edward/Sila/train_edu_bert.py", line 101, in preprocess
    batch = tokenizer(examples["code"], truncation=True)
  File "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3055, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3142, in _call_one
    return self.batch_encode_plus(
  File "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3338, in batch_encode_plus
    return self._batch_encode_plus(
  File "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 528, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
KeyboardInterrupt